import evaluate
import re
import numpy as np
import wandb
from datasets import load_dataset, DatasetDict
from transformers import (
    T5ForConditionalGeneration,
    T5Tokenizer,
    DataCollatorForSeq2Seq,
    Trainer,
    TrainingArguments,
)

MODEL_NAME = "google/flan-t5-base"
DATASET_NAME = "JuliaTsk/yahoo-answers"
NEW_MODEL = "JuliaTsk/FLAN-T5-base-finetune"


def preprocess_function(examples):
    """Add prefix to the sentences, tokenize the text, and set the labels"""
    prefix = "Please answer this question: "
    # The "inputs" are the tokenized questions from dataset:
    inputs = [prefix + qn for qn in examples["question title"]]
    model_inputs = tokenizer(inputs, max_length=128, truncation=True, padding="max_length")

    # The "labels" are the tokenized outputs:
    labels = tokenizer(text_target=[str(ans) if ans is not None else "" for ans in examples["best answer"]],
                       max_length=128, truncation=True, padding="max_length")
    model_inputs["labels"] = labels["input_ids"]
    return model_inputs


def compute_metrics(predictions, labels):
    """Compute BERT-score metric"""
    # Load BERTScore metric
    bertscore = evaluate.load("bertscore")
    # Compute BERTScore
    results = bertscore.compute(predictions=predictions, references=labels, lang="en")
    wandb.log({
        "bertscore": np.mean(results["f1"]),
    })
    return {"bertscore_f1": np.mean(results["f1"])}


# Acquire the training data from Hugging Face
data_files = {"train": "train.csv", "test": "test.csv"}
yahoo_answers_qa = load_dataset(DATASET_NAME, data_files=data_files)
train_subset = yahoo_answers_qa['train'].select(range(28000))
test_subset = yahoo_answers_qa['test'].select(range(8000))
yahoo_answers_qa = DatasetDict({
    "train": train_subset,
    "test": test_subset
})

# Load the tokenizer, model, and data collator
tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME)
model = T5ForConditionalGeneration.from_pretrained(MODEL_NAME)
data_collator = DataCollatorForSeq2Seq(
    tokenizer=tokenizer,
    model=model
)

# Map the preprocessing function across our dataset
tokenized_dataset = yahoo_answers_qa.map(preprocess_function, batched=True)

training_args = TrainingArguments(
    report_to='wandb',  # Track metrics
    eval_strategy='steps',  # Check evaluation
    num_train_epochs=1,  # Number of training loops
    output_dir="./output",
    logging_steps=100,
    eval_steps=100,
    save_steps=100,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=2,
    group_by_length=True,
    run_name='yahoo_training',
    logging_dir="./logs",
    gradient_accumulation_steps=1,
    fp16=True,  # Enable mixed precision training to avoid OutOfMemory errors
    optim="paged_adamw_32bit",  # Optimizer for training
    learning_rate=2e-4,  # Initial learning rate
    eval_accumulation_steps=1,
    weight_decay=0.001, # Weight decay to apply to all layers except bias/LayerNorm weights
    max_steps=1500
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"],
    compute_metrics=compute_metrics,
    data_collator=data_collator,
)

trainer.train()

""" Compare answers before and after Fine-Tuning """
# Example question
example = "why doesn't an optical mouse work on a glass table? "
example1 = "What's the capital of French?"
example2 = "Why does Zebras have stripes?"

model = T5ForConditionalGeneration.from_pretrained("google/flan-t5-base")
last_checkpoint = "output/checkpoint-1500"
finetuned_model = T5ForConditionalGeneration.from_pretrained(last_checkpoint)

input_ids = tokenizer(example, return_tensors="pt").input_ids
outputs = model.generate(input_ids)
cleaned_output = re.sub(r"<.*?>", "", tokenizer.decode(outputs[0])).strip()

# Generate answers
inputs = tokenizer(example, return_tensors="pt")
finetuned_answer = tokenizer.decode(finetuned_model.generate(**inputs)[0], skip_special_tokens=True)

print(f"Original Model Answer: {cleaned_output}")
print(f"Fine-Tuned Model Answer: {finetuned_answer}")

# Save the model for future testing
model.push_to_hub(NEW_MODEL, check_pr=True)
tokenizer.push_to_hub(NEW_MODEL,check_pr=True)
